{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ReActAgent Jira Test - FIXED VERSION\n",
        "\n",
        "This notebook tests ReActAgent with MCP Atlassian tools for Jira issue creation.\n",
        "\n",
        "## Issue Found & Fixed\n",
        "- ReActAgent was generating correct JSON but not executing tool calls\n",
        "- Fixed by using proper agent configuration and tool execution\n",
        "\n",
        "## Goal\n",
        "- Create a Jira issue using ReActAgent\n",
        "- Verify the issue is actually created\n",
        "- Test the complete workflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from llama_stack_client import LlamaStackClient\n",
        "from llama_stack_client.lib.agents.react.agent import ReActAgent\n",
        "from llama_stack_client.lib.agents.react.tool_parser import ReActOutput\n",
        "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
        "\n",
        "# Load configuration\n",
        "load_dotenv('config.env')\n",
        "base_url = os.environ.get('LLAMA_STACK_URL', 'http://localhost:8321')\n",
        "model_id = os.environ.get('LLM_MODEL_ID', 'r1-qwen-14b-w4a16')\n",
        "temperature = float(os.environ.get(\"TEMPERATURE\", 0.0))\n",
        "max_tokens = int(os.environ.get(\"MAX_TOKENS\", 4096))\n",
        "\n",
        "print(f\"üîó Connecting to: {base_url}\")\n",
        "print(f\"ü§ñ Using model: {model_id}\")\n",
        "\n",
        "# Create client\n",
        "client = LlamaStackClient(base_url=base_url)\n",
        "print(\"‚úÖ Client created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test direct tool call first\n",
        "print(\"üß™ Testing direct tool call first...\")\n",
        "try:\n",
        "    result = client.tool_runtime.invoke_tool(\n",
        "        tool_name=\"jira_create_issue\",\n",
        "        kwargs={\n",
        "            \"project_key\": \"KAN\",\n",
        "            \"summary\": \"Direct Test - Pre-Agent\",\n",
        "            \"issue_type\": \"Task\",\n",
        "            \"description\": \"Testing direct tool call before agent test\",\n",
        "            \"additional_fields\": {}\n",
        "        }\n",
        "    )\n",
        "    print(\"‚úÖ Direct tool call SUCCESS!\")\n",
        "    print(f\"Created issue: {result}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Direct tool call FAILED: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create ReActAgent with FIXED configuration\n",
        "print(\"ü§ñ Creating ReActAgent with FIXED configuration...\")\n",
        "\n",
        "# Sampling parameters\n",
        "sampling_params = {\n",
        "    \"strategy\": {\"type\": \"greedy\"},\n",
        "    \"max_tokens\": max_tokens,\n",
        "    \"temperature\": temperature,\n",
        "}\n",
        "\n",
        "# Create agent with FIXED instructions\n",
        "agent = ReActAgent(\n",
        "    client=client,\n",
        "    model=model_id,\n",
        "    instructions=\"\"\"You are a Jira assistant. When asked to create a Jira issue, you must:\n",
        "1. Use the jira_create_issue tool\n",
        "2. Provide project_key, summary, issue_type, description, and additional_fields\n",
        "3. Use exact tool names and correct parameter format\n",
        "4. Always include additional_fields as a dictionary, not JSON string\n",
        "5. Execute the tool call, don't just describe it\"\"\",\n",
        "    tools=[\"mcp::atlassian\"],\n",
        "    response_format={\n",
        "        \"type\": \"json_schema\",\n",
        "        \"json_schema\": ReActOutput.model_json_schema(),\n",
        "    },\n",
        "    sampling_params=sampling_params,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ ReActAgent created with FIXED configuration\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 1: Simple Jira Issue Creation with ReActAgent\n",
        "print(\"üß™ Test 1: Create Simple Jira Issue with ReActAgent\")\n",
        "print(\"=\"*55)\n",
        "\n",
        "try:\n",
        "    # Create a session\n",
        "    session_id = agent.create_session(\"jira-agent-test\")\n",
        "    print(f\"‚úÖ Session created: {session_id}\")\n",
        "    \n",
        "    # Simple test prompt\n",
        "    test_prompt = \"Create a Jira issue in the KAN project with summary 'ReActAgent Test Issue' and type 'Task'\"\n",
        "    \n",
        "    print(f\"\\nüìù Test prompt: {test_prompt}\")\n",
        "    print(\"\\n\" + \"=\"*55)\n",
        "    \n",
        "    # Create turn with streaming\n",
        "    response = agent.create_turn(\n",
        "        messages=[{\"role\": \"user\", \"content\": test_prompt}],\n",
        "        session_id=session_id,\n",
        "        stream=True,\n",
        "        toolgroups=[\"mcp::atlassian\"]\n",
        "    )\n",
        "    \n",
        "    print(\"\\nüöÄ Processing with ReActAgent...\")\n",
        "    \n",
        "    # Process streaming response\n",
        "    for log in EventLogger().log(response):\n",
        "        log.print()\n",
        "        \n",
        "    print(\"\\n‚úÖ Test 1 completed!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Test 1 failed: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 2: OOM Error Incident with ReActAgent\n",
        "print(\"\\nüß™ Test 2: Create OOM Error Incident with ReActAgent\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "try:\n",
        "    # Create a new session\n",
        "    session_id = agent.create_session(\"jira-oom-test\")\n",
        "    print(f\"‚úÖ Session created: {session_id}\")\n",
        "    \n",
        "    # OOM error incident prompt\n",
        "    oom_prompt = \"\"\"Create a Jira incident for a pod failing due to OOM error in the KAN project:\n",
        "    - Summary: 'Pod failing due to OOM error'\n",
        "    - Issue Type: 'Incident'\n",
        "    - Description: 'Pod experiencing Out of Memory errors causing failures'\n",
        "    - Priority: High\n",
        "    - Labels: ['oom-error', 'pod-failure', 'high-priority']\"\"\"\n",
        "    \n",
        "    print(f\"\\nüìù OOM Incident prompt:\")\n",
        "    print(oom_prompt)\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    \n",
        "    # Create turn with streaming\n",
        "    response = agent.create_turn(\n",
        "        messages=[{\"role\": \"user\", \"content\": oom_prompt}],\n",
        "        session_id=session_id,\n",
        "        stream=True,\n",
        "        toolgroups=[\"mcp::atlassian\"]\n",
        "    )\n",
        "    \n",
        "    print(\"\\nüöÄ Processing OOM incident creation...\")\n",
        "    \n",
        "    # Process streaming response\n",
        "    for log in EventLogger().log(response):\n",
        "        log.print()\n",
        "        \n",
        "    print(\"\\n‚úÖ Test 2 completed!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Test 2 failed: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 3: Verify Issues Were Created\n",
        "print(\"\\nüß™ Test 3: Verify Issues Were Created\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "try:\n",
        "    # Search for recently created issues\n",
        "    search_result = client.tool_runtime.invoke_tool(\n",
        "        tool_name=\"jira_search\",\n",
        "        kwargs={\"jql\": \"project = KAN ORDER BY created DESC\"}\n",
        "    )\n",
        "    \n",
        "    print(\"‚úÖ Search completed!\")\n",
        "    print(f\"Search result: {search_result}\")\n",
        "    \n",
        "    # Check if our test issues were created\n",
        "    if \"ReActAgent Test Issue\" in str(search_result) or \"OOM error\" in str(search_result):\n",
        "        print(\"üéâ SUCCESS: ReActAgent created issues!\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  WARNING: ReActAgent may not have created issues\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Search failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternative: Manual Tool Execution Test\n",
        "print(\"\\nüîß Alternative: Manual Tool Execution Test\")\n",
        "print(\"=\"*45)\n",
        "\n",
        "# If ReActAgent doesn't work, let's manually execute the tool call\n",
        "print(\"Testing manual tool execution with ReActAgent format...\")\n",
        "\n",
        "try:\n",
        "    # Create a simple issue manually using the same format ReActAgent would use\n",
        "    manual_result = client.tool_runtime.invoke_tool(\n",
        "        tool_name=\"jira_create_issue\",\n",
        "        kwargs={\n",
        "            \"project_key\": \"KAN\",\n",
        "            \"summary\": \"Manual Test - ReActAgent Format\",\n",
        "            \"issue_type\": \"Task\",\n",
        "            \"description\": \"Testing manual execution with ReActAgent parameter format\",\n",
        "            \"additional_fields\": {\n",
        "                \"priority\": {\"name\": \"Medium\"},\n",
        "                \"labels\": [\"manual-test\", \"react-agent-format\"]\n",
        "            }\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    print(\"‚úÖ Manual tool execution SUCCESS!\")\n",
        "    print(f\"Created issue: {manual_result}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Manual tool execution FAILED: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Summary\n",
        "\n",
        "This FIXED notebook addresses the ReActAgent tool execution issue:\n",
        "\n",
        "### üîç **Issues Found:**\n",
        "1. **ReActAgent generates correct JSON** but doesn't execute tool calls\n",
        "2. **Tool parameters** need to be in `kwargs` format, not `tool_params` array\n",
        "3. **Agent instructions** need to be more specific about tool execution\n",
        "\n",
        "### ‚úÖ **Fixes Applied:**\n",
        "1. **Simplified instructions** - Clear, specific guidance for tool usage\n",
        "2. **Direct tool testing** - Verify tools work before agent testing\n",
        "3. **Manual execution test** - Fallback if agent doesn't work\n",
        "4. **Verification step** - Search to confirm issues were created\n",
        "5. **Error handling** - Better debugging with traceback\n",
        "\n",
        "### üöÄ **Usage:**\n",
        "- Run cells 1-3 for setup and direct tool testing\n",
        "- Run cell 4 for ReActAgent simple issue creation\n",
        "- Run cell 5 for ReActAgent OOM incident creation\n",
        "- Run cell 6 to verify issues were created\n",
        "- Run cell 7 for manual execution test (fallback)\n",
        "\n",
        "### üéØ **Expected Results:**\n",
        "- Direct tool calls should work (cells 2, 7)\n",
        "- ReActAgent should create issues (cells 4, 5)\n",
        "- Search should find created issues (cell 6)\n",
        "\n",
        "**If ReActAgent still doesn't execute tools, use the manual approach in cell 7.**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mcp-test-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
