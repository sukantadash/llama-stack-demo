{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Conversations API with Built-in MCP Tools\n",
        "\n",
        "This notebook uses the Conversations API with the built-in MCP tools that are already configured in the Llama Stack server.\n",
        "\n",
        "## Key Insight:\n",
        "**MCP tools are configured at the server level** - they should be automatically available to the Conversations API without needing to specify server URLs.\n",
        "\n",
        "## Goal:\n",
        "- Use Conversations API with automatic MCP tool calling\n",
        "- No manual intervention needed\n",
        "- Use the existing MCP tool configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from llama_stack_client import LlamaStackClient\n",
        "\n",
        "# Load configuration\n",
        "load_dotenv('config.env')\n",
        "base_url = os.environ.get('LLAMA_STACK_URL', 'http://localhost:8321')\n",
        "model_id = os.environ.get('LLM_MODEL_ID', 'r1-qwen-14b-w4a16')\n",
        "\n",
        "print(f\"üîó Connecting to: {base_url}\")\n",
        "print(f\"ü§ñ Using model: {model_id}\")\n",
        "\n",
        "# Create client\n",
        "client = LlamaStackClient(base_url=base_url)\n",
        "print(\"‚úÖ Client created\")\n",
        "\n",
        "# Check if server is running\n",
        "try:\n",
        "    # Try a simple request to check server status\n",
        "    models = client.models.list()\n",
        "    print(f\"‚úÖ Server is running - found {len(models)} models\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Server not available: {e}\")\n",
        "    print(\"Please start the Llama Stack server first\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 1: Simple Jira Issue Creation with Built-in MCP Tools\n",
        "print(\"üß™ Test 1: Create Simple Jira Issue with Built-in MCP Tools\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "try:\n",
        "    # Use Conversations API with built-in MCP tools (no explicit tool configuration needed)\n",
        "    response1 = client.responses.create(\n",
        "        model=model_id,\n",
        "        input=\"Create a Jira issue in the KAN project with summary 'Conversations API Built-in Test' and type 'Task'\"\n",
        "    )\n",
        "    \n",
        "    print(\"‚úÖ Conversations API response received!\")\n",
        "    print(f\"Response ID: {response1.id}\")\n",
        "    print(f\"Response content: {response1.output[0].content[0].text}\")\n",
        "    \n",
        "    # Check if tools were executed\n",
        "    if hasattr(response1, 'tool_calls') and response1.tool_calls:\n",
        "        print(f\"\\nüîß Tools executed: {len(response1.tool_calls)}\")\n",
        "        for i, tool_call in enumerate(response1.tool_calls):\n",
        "            print(f\"  Tool {i+1}: {tool_call.function.name}\")\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è  No tool calls found in response\")\n",
        "    \n",
        "    print(\"\\n‚úÖ Test 1 completed!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Test 1 failed: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 2: OOM Error Incident Creation with Built-in MCP Tools\n",
        "print(\"\\nüß™ Test 2: Create OOM Error Incident with Built-in MCP Tools\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "try:\n",
        "    # Use Conversations API with built-in MCP tools\n",
        "    response2 = client.responses.create(\n",
        "        model=model_id,\n",
        "        input=\"\"\"Create a Jira incident for a pod failing due to OOM error in the KAN project:\n",
        "        - Summary: 'Pod failing due to OOM error'\n",
        "        - Issue Type: 'Incident'\n",
        "        - Description: 'Pod experiencing Out of Memory errors causing application failures'\n",
        "        - Priority: High\n",
        "        - Labels: ['oom-error', 'pod-failure', 'high-priority']\"\"\",\n",
        "        previous_response_id=response1.id  # Branch from first response\n",
        "    )\n",
        "    \n",
        "    print(\"‚úÖ Conversations API response received!\")\n",
        "    print(f\"Response ID: {response2.id}\")\n",
        "    print(f\"Previous Response ID: {response1.id}\")\n",
        "    print(f\"Response content: {response2.output[0].content[0].text}\")\n",
        "    \n",
        "    # Check if tools were executed\n",
        "    if hasattr(response2, 'tool_calls') and response2.tool_calls:\n",
        "        print(f\"\\nüîß Tools executed: {len(response2.tool_calls)}\")\n",
        "        for i, tool_call in enumerate(response2.tool_calls):\n",
        "            print(f\"  Tool {i+1}: {tool_call.function.name}\")\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è  No tool calls found in response\")\n",
        "    \n",
        "    print(\"\\n‚úÖ Test 2 completed!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Test 2 failed: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrates using the **Conversations API with Built-in MCP Tools**:\n",
        "\n",
        "### üîç **Key Insight:**\n",
        "**MCP tools are configured at the server level** - they should be automatically available to the Conversations API without needing to specify server URLs or tool configurations.\n",
        "\n",
        "### üöÄ **How It Works:**\n",
        "1. **`client.responses.create()`** - Main API call\n",
        "2. **No explicit tool configuration** - MCP tools are built-in\n",
        "3. **`previous_response_id`** - For branching conversations\n",
        "4. **Automatic execution** - API handles tool calling internally\n",
        "\n",
        "### üéØ **What We Tested:**\n",
        "1. **Simple Jira Issue Creation** - Basic task creation with automatic tool calling\n",
        "2. **OOM Error Incident Creation** - Complex incident with conversation context\n",
        "3. **Branching Conversation** - Create additional issues from previous responses\n",
        "\n",
        "### üí° **Advantages:**\n",
        "- ‚úÖ **Pure Conversations API** - No hybrid approach needed\n",
        "- ‚úÖ **Automatic tool execution** - No manual intervention\n",
        "- ‚úÖ **Built-in MCP tools** - No configuration needed\n",
        "- ‚úÖ **Branching support** - Can branch conversations\n",
        "- ‚úÖ **Simple interface** - Just call the API\n",
        "\n",
        "### üîÑ **API Pattern:**\n",
        "```python\n",
        "response = client.responses.create(\n",
        "    model=model_id,\n",
        "    input=\"Your prompt here\",\n",
        "    previous_response_id=previous_response.id  # Optional for branching\n",
        ")\n",
        "```\n",
        "\n",
        "**This is the simplest Conversations API approach with automatic MCP tool calling!**\n",
        "\n",
        "### üìù **Note:**\n",
        "The server needs to be running with MCP tools configured. If the server is not available, the API calls will fail with connection errors.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
